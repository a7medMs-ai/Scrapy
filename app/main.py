import streamlit as st
import os
import pandas as pd
from io import BytesIO
import requests
from bs4 import BeautifulSoup
from zipfile import ZipFile
from datetime import datetime

def load_custom_css():
    css_path = os.path.join(os.path.dirname(__file__), '../assets/style.css')
    if os.path.exists(css_path):
        with open(css_path) as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

def fetch_html_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        st.error(f"Failed to fetch the page: {e}")
        return None

def analyze_html(html):
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text()
    words = text.split()
    return {
        "Word Count": len(words),
        "Title": soup.title.string if soup.title else "N/A",
        "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

def convert_df_to_excel(df):
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False)
    output.seek(0)
    return output

def save_html_as_zip(html_content, url):
    zip_io = BytesIO()
    with ZipFile(zip_io, 'w') as zip_file:
        filename = url.replace("https://", "").replace("http://", "").replace("/", "_") + ".html"
        zip_file.writestr(filename, html_content)
    zip_io.seek(0)
    return zip_io

def main():
    st.set_page_config(page_title="Scrapy Localization Tool", layout="wide")
    load_custom_css()

    st.title("üåê Website Scraping & Localization Estimator Tool")

    st.markdown("""
        **Instructions:**
        - Enter a full website URL to extract word/segment count and metadata.
        - Download Excel or HTML ZIP once analysis is complete.
    """)

    url = st.text_input("üîó Enter full website URL", placeholder="https://example.com")
    run_analysis = st.button("üîç Analyze Website")

    if run_analysis and url:
        with st.spinner("Fetching and analyzing..."):
            html = fetch_html_from_url(url)
            if html:
                results = analyze_html(html)
                df = pd.DataFrame([results])
                excel_data = convert_df_to_excel(df)
                zip_data = save_html_as_zip(html, url)

                st.success(f"‚úÖ Analysis complete for: {url}")
                st.dataframe(df)

                col1, col2, col3 = st.columns([1, 1, 2])
                with col1:
                    st.download_button("üì• Download Excel Report", data=excel_data,
                                       file_name="report.xlsx",
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                with col2:
                    st.download_button("üì¶ Download HTML ZIP", data=zip_data,
                                       file_name="html_page.zip", mime="application/zip")
                with col3:
                    st.button("üîÑ Start New Session")

if __name__ == "__main__":
    main()
